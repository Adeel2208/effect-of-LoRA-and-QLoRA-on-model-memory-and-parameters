{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-07T11:28:44.842704Z","iopub.execute_input":"2024-08-07T11:28:44.843525Z","iopub.status.idle":"2024-08-07T11:28:44.849221Z","shell.execute_reply.started":"2024-08-07T11:28:44.843493Z","shell.execute_reply":"2024-08-07T11:28:44.848285Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:28:48.929179Z","iopub.execute_input":"2024-08-07T11:28:48.929527Z","iopub.status.idle":"2024-08-07T11:28:58.699847Z","shell.execute_reply.started":"2024-08-07T11:28:48.929500Z","shell.execute_reply":"2024-08-07T11:28:58.698959Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dadddb8e0004934a19637eb9e0ffa91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/497k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f73012a6bd4d42a5802c3c7c26d37a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/277k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e19681e71a7479a97e69c84d6147de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/840k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad0bc40ce6c4b5abfb788aea917337a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8850221f3f4f4f8af9ba9d271b6c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/903 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"206378c1be6a4f38bfccbc8396241caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/457M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1524420b1e134722a6d5bf75840715a1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndef count_parameters_and_memory(model):\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_memory = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return total_params, total_memory\n\ndef format_memory_size(bytes_size):\n    # Convert bytes to MB or GB\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if bytes_size < 1024:\n            return f\"{bytes_size:.2f} {unit}\"\n        bytes_size /= 1024\n\n# Assuming 'model' is the variable holding your LLM\nmodel_parameters, model_memory = count_parameters_and_memory(model)\nformatted_memory = format_memory_size(model_memory)\n\nprint(f\"The model has {model_parameters:,} parameters.\")\nprint(f\"The model is taking approximately {formatted_memory} of memory.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:35:37.594442Z","iopub.execute_input":"2024-08-07T11:35:37.594805Z","iopub.status.idle":"2024-08-07T11:35:37.604698Z","shell.execute_reply.started":"2024-08-07T11:35:37.594777Z","shell.execute_reply":"2024-08-07T11:35:37.603778Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The model has 111,008,256 parameters.\nThe model is taking approximately 423.46 MB of memory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# reducing prescision","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n\n# Load model in float16\nmodel1 = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\", torch_dtype=torch.float16)\n\ndef count_parameters_and_memory(model1):\n    total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n    total_memory = sum(p.numel() * p.element_size() for p in model1.parameters() if p.requires_grad)\n    return total_params, total_memory\n\ndef format_memory_size(bytes_size):\n    # Convert bytes to MB or GB\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if bytes_size < 1024:\n            return f\"{bytes_size:.2f} {unit}\"\n        bytes_size /= 1024\n\n# Assuming 'model' is the variable holding your LLM\nmodel_parameters, model_memory = count_parameters_and_memory(model1)\nformatted_memory = format_memory_size(model_memory)\n\nprint(f\"The model has {model_parameters:,} parameters.\")\nprint(f\"The model is taking approximately {formatted_memory} of memory.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:36:34.135638Z","iopub.execute_input":"2024-08-07T11:36:34.136025Z","iopub.status.idle":"2024-08-07T11:36:34.914968Z","shell.execute_reply.started":"2024-08-07T11:36:34.135972Z","shell.execute_reply":"2024-08-07T11:36:34.914007Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The model has 111,008,256 parameters.\nThe model is taking approximately 211.73 MB of memory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# applying QLoRA","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom peft import LoraConfig, get_peft_model\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n\n# Load model in float16\nmodel = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\", torch_dtype=torch.float16)\n\n# Apply dynamic quantization\nmodel = torch.quantization.quantize_dynamic(\n    model,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8  # dtype of quantized weights\n)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n      lora_alpha=16,\n      lora_dropout=0.1,\n      r=64,\n      bias=\"none\",\n      task_type=\"CAUSAL_LM\",\n)\nLoraConfig()\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\ndef count_parameters_and_memory(model):\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_memory = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return total_params, total_memory\n\ndef format_memory_size(bytes_size):\n    # Convert bytes to MB or GB\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if bytes_size < 1024:\n            return f\"{bytes_size:.2f} {unit}\"\n        bytes_size /= 1024\n\n# Calculate the number of parameters and memory usage of the quantized model\nmodel_parameters, model_memory = count_parameters_and_memory(model)\nformatted_memory = format_memory_size(model_memory)\n\nprint(f\"The LoRA-adapted and quantized model has {model_parameters:,} parameters.\")\nprint(f\"The LoRA-adapted and quantized model is taking approximately {formatted_memory} of memory.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:18:02.576132Z","iopub.execute_input":"2024-08-07T19:18:02.577123Z","iopub.status.idle":"2024-08-07T19:18:03.917654Z","shell.execute_reply.started":"2024-08-07T19:18:02.577084Z","shell.execute_reply":"2024-08-07T19:18:03.916636Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The LoRA-adapted and quantized model has 2,359,296 parameters.\nThe LoRA-adapted and quantized model is taking approximately 9.00 MB of memory.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# reducing rank of model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom peft import LoraConfig, get_peft_model\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n\n# Load model in float16\nmodel = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\", torch_dtype=torch.float16)\n\n# Apply dynamic quantization\nmodel = torch.quantization.quantize_dynamic(\n    model,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8  # dtype of quantized weights\n)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n      lora_alpha=16,\n      lora_dropout=0.1,\n      r=32,\n      bias=\"none\",\n      task_type=\"CAUSAL_LM\",\n)\nLoraConfig()\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\ndef count_parameters_and_memory(model):\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_memory = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return total_params, total_memory\n\ndef format_memory_size(bytes_size):\n    # Convert bytes to MB or GB\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if bytes_size < 1024:\n            return f\"{bytes_size:.2f} {unit}\"\n        bytes_size /= 1024\n\n# Calculate the number of parameters and memory usage of the quantized model\nmodel_parameters, model_memory = count_parameters_and_memory(model)\nformatted_memory = format_memory_size(model_memory)\n\nprint(f\"The LoRA-adapted and quantized model has {model_parameters:,} parameters.\")\nprint(f\"The LoRA-adapted and quantized model is taking approximately {formatted_memory} of memory.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:20:41.541558Z","iopub.execute_input":"2024-08-07T19:20:41.542480Z","iopub.status.idle":"2024-08-07T19:20:42.745552Z","shell.execute_reply.started":"2024-08-07T19:20:41.542443Z","shell.execute_reply":"2024-08-07T19:20:42.744635Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The LoRA-adapted and quantized model has 1,179,648 parameters.\nThe LoRA-adapted and quantized model is taking approximately 4.50 MB of memory.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom peft import LoraConfig, get_peft_model\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n\n# Load model in float16\nmodel = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\", torch_dtype=torch.float16)\n\n# Apply dynamic quantization\nmodel = torch.quantization.quantize_dynamic(\n    model,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8  # dtype of quantized weights\n)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n      lora_alpha=16,\n      lora_dropout=0.1,\n      r=8,\n      bias=\"none\",\n      task_type=\"CAUSAL_LM\",\n)\nLoraConfig()\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\ndef count_parameters_and_memory(model):\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_memory = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return total_params, total_memory\n\ndef format_memory_size(bytes_size):\n    # Convert bytes to MB or GB\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if bytes_size < 1024:\n            return f\"{bytes_size:.2f} {unit}\"\n        bytes_size /= 1024\n\n# Calculate the number of parameters and memory usage of the quantized model\nmodel_parameters, model_memory = count_parameters_and_memory(model)\nformatted_memory = format_memory_size(model_memory)\n\nprint(f\"The LoRA-adapted and quantized model has {model_parameters:,} parameters.\")\nprint(f\"The LoRA-adapted and quantized model is taking approximately {formatted_memory} of memory.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:25:35.528733Z","iopub.execute_input":"2024-08-07T19:25:35.529830Z","iopub.status.idle":"2024-08-07T19:25:36.685556Z","shell.execute_reply.started":"2024-08-07T19:25:35.529778Z","shell.execute_reply":"2024-08-07T19:25:36.684630Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The LoRA-adapted and quantized model has 294,912 parameters.\nThe LoRA-adapted and quantized model is taking approximately 1.12 MB of memory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# rank=4","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom peft import LoraConfig, get_peft_model\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n\n# Load model in float16\nmodel = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\", torch_dtype=torch.float16)\n\n# Apply dynamic quantization\nmodel = torch.quantization.quantize_dynamic(\n    model,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8  # dtype of quantized weights\n)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n      lora_alpha=16,\n      lora_dropout=0.1,\n      r=4,\n      bias=\"none\",\n      task_type=\"CAUSAL_LM\",\n)\nLoraConfig()\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\ndef count_parameters_and_memory(model):\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_memory = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return total_params, total_memory\n\ndef format_memory_size(bytes_size):\n    # Convert bytes to MB or GB\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if bytes_size < 1024:\n            return f\"{bytes_size:.2f} {unit}\"\n        bytes_size /= 1024\n\n# Calculate the number of parameters and memory usage of the quantized model\nmodel_parameters, model_memory = count_parameters_and_memory(model)\nformatted_memory = format_memory_size(model_memory)\n\nprint(f\"The LoRA-adapted and quantized model has {model_parameters:,} parameters.\")\nprint(f\"The LoRA-adapted and quantized model is taking approximately {formatted_memory} of memory.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:26:00.334841Z","iopub.execute_input":"2024-08-07T19:26:00.335215Z","iopub.status.idle":"2024-08-07T19:26:01.506696Z","shell.execute_reply.started":"2024-08-07T19:26:00.335186Z","shell.execute_reply":"2024-08-07T19:26:01.505779Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"The LoRA-adapted and quantized model has 147,456 parameters.\nThe LoRA-adapted and quantized model is taking approximately 576.00 KB of memory.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}